const axios = require('axios');
const { SpeechClient } = require('@google-cloud/speech').v1;
const { ImageAnnotatorClient } = require('@google-cloud/vision').v1;
const FormData = require('form-data');
const fs = require('fs');
const path = require('path');

// Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª gRPC Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
process.env.GRPC_DNS_RESOLVER = 'native';
process.env.GRPC_VERBOSITY = 'DEBUG';

class AIAgent {
  constructor() {
    // Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª APIs
    this.elevenLabsKey = process.env.ELEVENLABS_KEY;
    this.openRouterKey = process.env.OPENROUTER_API_KEY;
    this.groqKey = process.env.GROQ_API_KEY;
    this.removeBgKey = process.env.REMOVEBG_KEY;

    // Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    this.sessions = {};

    // Ø¹Ù…Ù„Ø§Ø¡ Google Cloud
    this.speechClient = this._initializeGoogleClient('speech');
    this.visionClient = this._initializeGoogleClient('vision');
  }

  _initializeGoogleClient() {
  try {
    // Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…ÙØ¶Ù„Ø© - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ù…Ø¨Ø§Ø´Ø±
    const credsPath = path.join(__dirname, '..', 'google-credentials.json');
    if (fs.existsSync(credsPath)) {
      const credentials = require(credsPath);
      return new SpeechClient({ credentials });
    }

    // Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø©
    if (process.env.GOOGLE_CREDENTIALS) {
      const fixedJson = process.env.GOOGLE_CREDENTIALS
        .replace(/\\n/g, '\n')
        .replace(/\\"/g, '"');
      
      return new SpeechClient({ 
        credentials: JSON.parse(fixedJson) 
      });
    }

    throw new Error('Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯');
  } catch (error) {
    console.error('âŒ ÙØ´Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©:', error.message);
    console.log('ğŸ” ØªØ£ÙƒØ¯ Ù…Ù†:');
    console.log('1. ØµØ­Ø© ØªÙ†Ø³ÙŠÙ‚ JSON (Ø¬Ø±Ø¨ ÙÙŠ jsonlint.com)');
    console.log('2. Ø£Ù† private_key ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ \\n ÙˆÙ„ÙŠØ³ Ø£Ø³Ø·Ø± Ø¬Ø¯ÙŠØ¯Ø© ÙØ¹Ù„ÙŠØ©');
    console.log('3. Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø£Ø­Ø±Ù Ø®Ø§ØµØ© ÙÙŠ Ø§Ù„Ù†Øµ');
    process.exit(1);
  }
}

  // ============== Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ==============

  async speechToText(audioBytes, languageCode = 'ar-SA') {
    try {
      const [response] = await this.speechClient.recognize({
        audio: { content: audioBytes },
        config: {
          encoding: 'WEBM_OPUS',
          sampleRateHertz: 48000,
          languageCode: languageCode,
          model: 'latest_long'
        }
      });
      return response.results
        .map(result => result.alternatives[0]?.transcript || '')
        .join('\n');
    } catch (error) {
      console.error('âŒ ÙØ´Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ:', error);
      throw new Error('Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª');
    }
  }

  async textToSpeech(text, voiceId = '21m00Tcm4TlvDq8ikWAM') {
    try {
      const response = await axios.post(
        `https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`,
        { 
          text,
          voice_settings: { 
            stability: 0.5, 
            similarity_boost: 0.5 
          }
        },
        {
          headers: { 
            'xi-api-key': this.elevenLabsKey,
            'Content-Type': 'application/json'
          },
          responseType: 'arraybuffer'
        }
      );
      return response.data;
    } catch (error) {
      console.error('âŒ ÙØ´Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª:', error.response?.data || error.message);
      throw new Error('Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª');
    }
  }

  // ... (Ø¨Ù‚ÙŠØ© Ø§Ù„ÙˆØ¸Ø§Ø¦Ù ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ Ø¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±)

  async handleRequest(sessionId, userInput, inputType = 'text') {
    if (!this.sessions[sessionId]) {
      this.sessions[sessionId] = { chatHistory: [], preferences: {} };
    }
    const session = this.sessions[sessionId];

    let userText = userInput;
    if (inputType === 'audio') {
      userText = await this.speechToText(userInput);
    }

    const task = this.detectTask(userText);
    let output;

    switch (task) {
      case 'code_generation':
        output = await this.generateCode(userText, this.extractLanguage(userText));
        break;
      case 'text_to_speech':
        output = await this.textToSpeech(userText);
        break;
      case 'remove_background':
        output = await this.removeBackground(userInput);
        break;
      default:
        session.chatHistory.push({ role: 'user', content: userText });
        output = await this.chatWithAI(session.chatHistory);
        session.chatHistory.push({ role: 'assistant', content: output });
    }

    return {
      output,
      outputType: task === 'text_to_speech' ? 'audio' : 
                  task === 'remove_background' ? 'image' : 'text'
    };
  }
}

module.exports = AIAgent;